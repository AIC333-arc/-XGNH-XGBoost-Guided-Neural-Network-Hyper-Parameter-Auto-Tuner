from xgboost import XGBRegressor as xgb
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
import numpy as np

# X, y = ... - input your X and y values here

X_clean = X.dropna()

y_clean = y.loc[X_clean.index]

X_train_full, X_test, y_train_full, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

hidden_sizes_1 = list(range(1,512))
num_hidden_layers_1 = list(range(1, 32))
batch_size_1 = [2**i for i in range(2, 8) if 2**i < X.shape[0]]  # typically 4,8,...128
learning_rate_1 = np.logspace(-4, -2, num=1000)
num_epochs_1 = list(range(20, 100))
activations = ["relu", "tanh"]
num_hidden_layers_1 = list(range(1, 30))
dropout_1 = np.linspace(0, 0.9, num=10)

def build_model(input_size, hidden_size, activation, num_hidden_layers, dropout, output_size):
 if activation == 'relu':
      act_fn = nn.ReLU()
 elif activation == 'tanh':
    act_fn = nn.Tanh()
 else:
    raise ValueError("Unsupported activation: choose 'relu' or 'tanh'")

 layers = []
 # First hidden layer
 layers.append(nn.Linear(input_size, hidden_size))
 layers.append(act_fn)
 layers.append(nn.Dropout(dropout))

 # Additional hidden layers
 for z in range(num_hidden_layers - 1):
     layers.append(nn.Linear(hidden_size, hidden_size))
     layers.append(act_fn)
     layers.append(nn.Dropout(dropout))

# Output layer
 layers.append(nn.Linear(hidden_size, output_size))

 return nn.Sequential(*layers)

def train_and_eval(params, patience=5):
    model = build_model(X.shape[1], 
                        params["hidden_size"], 
                        params["activation"], 
                        params["num_hidden_layers"], 
                        params["dropout"], 
                        output_size=1)
    
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=params["learning_rate"])

    train_dataset = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), 
                                  torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))
    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'])

    val_tensor_X = torch.tensor(X_val.values, dtype=torch.float32)
    val_tensor_y = torch.tensor(y_val.values, dtype=torch.float32)

    best_rmse = float('inf')
    epochs_no_improve = 0
    best_model_state = None

    model.train()
    for epoch in range(params["num_epochs"]):
        for xb, yb in train_loader:
            optimizer.zero_grad()
            preds = model(xb)
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()

        # Validation RMSE
        model.eval()
        with torch.no_grad():
            preds = model(val_tensor_X).squeeze(1)
            mse = ((preds - val_tensor_y) ** 2).mean().item()
            rmse = mse ** 0.5

        if rmse < best_rmse:
            best_rmse = rmse
            epochs_no_improve = 0
            best_model_state = model.state_dict()
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

        model.train()

    # Restore best model before returning
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    return best_rmse

def encode_params(df):
  df_enc = df.copy()
  df_enc["activation"] = df_enc["activation"].map({"relu" : 0, "tanh" : 1})
  return df_enc

results_df = pd.DataFrame(columns = [
    "hidden_size", "learning_rate", "num_epochs", "batch_size", "num_hidden_layers", "activation", "dropout"
])

for i in range(100):

    hp = {
        "hidden_size" : random.choice(hidden_sizes_1),
        "learning_rate" : random.choice(learning_rate_1),
        "num_epochs" : random.choice(num_epochs_1),
        "batch_size" : random.choice(batch_size_1),
        "num_hidden_layers" : random.choice(num_hidden_layers_1),
        "activation" : random.choice(activations),
        "dropout" : random.choice(dropout_1),
    }
    
    # Skip if batch_size is invalid
    if hp["batch_size"] is None or hp["batch_size"] <= 0:
        continue

    acc = train_and_eval(hp)
    hp["rmse"] = acc
    results_df = pd.concat([results_df, pd.DataFrame([hp])], ignore_index=True)
    
    print(f"Initial sample: {hp}")



for iteration in range(100):
  df_enc = encode_params(results_df)
  df_enc = df_enc.apply(pd.to_numeric, errors='coerce').dropna(subset=['rmse', 'batch_size'])


  X_train_surrogate = df_enc.drop(columns=["rmse"])
  y_train_surrogate = df_enc["rmse"]

  if len(X_train_surrogate) == 0:
      print("Skipping surrogate training: no valid data.")
      continue

  surrogate = xgb(objective="reg:squarederror")
  surrogate.fit(X_train_surrogate, y_train_surrogate)


  candidates = []

  for f in range(100):
    candidate = {
      "hidden_size" : random.choice(hidden_sizes_1),
      "learning_rate" : random.choice(learning_rate_1),
      "num_epochs" : random.choice(num_epochs_1),
      "batch_size" : random.choice(batch_size_1),
      "num_hidden_layers" : random.choice(num_hidden_layers_1),
      "activation" : random.choice(activations),
      "dropout" : random.choice(dropout_1),

  }

    candidates.append(candidate)
  if candidate["batch_size"] is None or candidate["batch_size"] <= 0:
    continue
  
  candidates_df = pd.DataFrame(candidates)

  # You encode 'activation' in training surrogate data but didn't encode in candidates
  candidates_enc = candidates_df.copy()
  candidates_enc["activation"] = candidates_enc["activation"].map({"relu":0, "tanh":1})

  # Ensure column order matches training data
  candidates_enc = candidates_enc[X_train_surrogate.columns]
  preds = surrogate.predict(candidates_enc.values)

  best_idex = np.argmin(preds)
  best_hp = candidates[best_idex]

  best_acc = train_and_eval(best_hp)
  best_hp["rmse"] = best_acc  # Convert to Python float
  
  results_df = pd.concat([results_df, pd.DataFrame([best_hp])], ignore_index=True)
  results_df = results_df.dropna(subset=['rmse', 'batch_size'])
  # print(f"Iteration {iteration+1} - Tested {best_hp}, acccuracy : {best_acc:.4f}")

valid_results = results_df.dropna(subset=['rmse', 'batch_size'])
best_row = valid_results.loc[valid_results['rmse'].idxmin()]

print("Best hyperparameters found:")
print(best_row.to_dict())

lr = candidate["learning_rate"]
hs = candidate["hidden_size"]
ne = candidate["num_epochs"]
bs = candidate["batch_size"]
nhl = candidate["num_hidden_layers"]
act = candidate["activation"]
drop = candidate["dropout"]


# --- YOU: Set your hyperparameters here ---
hidden_size = hs
learning_rate = lr
num_epochs = ne
batch_size = bs
activation = act  # or "tanh"
num_hidden_layers = nhl
dropout = drop

# --- Data Prep (Assumes X and y are already defined) ---
X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)
train_ds = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32),
                         torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

# --- Final Evaluation on the test set ---
model = build_model(X.shape[1], hidden_size, activation, num_hidden_layers, dropout, output_size=1)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.MSELoss()

train_ds = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32),
                         torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

# Train on full training set (no early stopping here)
model.train()
for epoch in range(num_epochs):
    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()

# Evaluate on the untouched test set
model.eval()
with torch.no_grad():
    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)
    preds = model(X_test_tensor).squeeze(1)
    mse = ((preds - y_test_tensor) ** 2).mean().item()
    rmse = mse ** 0.5

print(f"Final RMSE on test set: {rmse:.4f}")

import sys
import pandas as pd
import torch

def load_dataset_from_file(feature_names):
    file_path = input("Enter the path to the CSV file containing your dataset: ").strip()
    
    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        print(f"Failed to read the CSV file: {e}")
        sys.exit(1)

    # Check if all required feature columns are present
    missing_cols = [col for col in feature_names if col not in df.columns]
    if missing_cols:
        print(f"Missing columns in the dataset: {missing_cols}")
        sys.exit(1)

    # Extract only the features needed
    features_df = df[feature_names]

    # Optional: Validate no missing values
    if features_df.isnull().any().any():
        print("Dataset contains missing values. Please clean the data before prediction.")
        sys.exit(1)
    
    return features_df.values.tolist()

# Your feature names from your original dataset
feature_names = X_clean.columns.tolist()

# Load dataset from user file
input_dataset = load_dataset_from_file(feature_names)

# Convert to tensor and predict
model.eval()
with torch.no_grad():
    input_tensor = torch.tensor(input_dataset, dtype=torch.float32)
    predictions = model(input_tensor)

print("\nPredictions:")
for i, pred in enumerate(predictions):
    print(f"Row {i + 1}: {pred.item():.4f}")
